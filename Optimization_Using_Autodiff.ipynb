{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton-type Optimization: Unconstrained Optimization\n",
    "Author: Suhan Shety, suhan.n.shetty@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook implements several Newton-type optimization algorithms using tensorflow 2.0. They are iterative in nature.\n",
    "\n",
    "Suppose we have,\n",
    "\n",
    "Forward model:  $\\mathit{M}(x): \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$, \n",
    "\n",
    "Target vector:   $Y \\in \\mathbb{R}^n $,\n",
    "\n",
    "The fit error vector:  $\\mathit{F}(x) = Y-\\mathit{M}(x)$,\n",
    "\n",
    "Squared Error:  $\\mathit{f}(x) = ||\\mathit{F}(x)|| $\n",
    "\n",
    "\n",
    "An update in the optimization parameter in the Newton-like method is given by: $$x_{k+1} = x_{k} - B_{k}^{-1} \\nabla f(x_k)$$\n",
    "\n",
    "The Newton-type methods differ by approximations to the Hessian matrix $B$.\n",
    "\n",
    "**Exact Newton(EN)**: uses the exact Hessian. So, its expensive computationally,\n",
    "        $$B_{k} = \\nabla ^{2} f(x_k)$$\n",
    "\n",
    "**Gradient Descent(GD)**:\n",
    "        $$B_{k} = \\alpha \\mathit{I},$$ $$ \\text{where $\\alpha$ is learning rate} $$\n",
    "\n",
    "\n",
    "**Gauss-Newton(GN)**:\n",
    "         $$B = J^{T}J$$\n",
    "         $$\\text{where $J = \\mathit{D} F(x_k)$ is the Jacobian matrix}$$\n",
    "         \n",
    "**Levenberg-Larquardt(LM)**: $$ B = J^{T}J + \\alpha \\mathit{I} $$\n",
    "      where $J = \\mathit{D} F(x_k)$  is the Jacobian matrix, \n",
    "         and $\\alpha$ is a regularizer. $\\alpha=0$ results in GN method and $\n",
    "         \\alpha >> 1$ results in GD\n",
    "         \n",
    "**BFGS:**   Approximates the hessians using a history of gradients\n",
    "    $$B_{k+1} = B_k - \\frac{s_k'B_k'B_k s_k}{s_k' B_k s} + \\frac{y_k y_k'}{s_k'y_k}$$\n",
    "    \n",
    "where $s_k = x_{k+1}-x_{k}$, and $y_k = \\nabla f(x_{k+1})-\\nabla f(x_{k})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data and choose a model\n",
    "N = 200 # Number of data points\n",
    "m = 20 # number of parameters (to learn)\n",
    "n = 3 # input dimension (state)\n",
    "\n",
    "# Model parameters (actual)\n",
    "x0 = tf.random.uniform((m,1),minval=0, maxval=1)\n",
    "\n",
    "# Model Input (state)\n",
    "Theta = tf.random.uniform((N,n),minval=0, maxval=1) \n",
    "t0 = tf.reshape(Theta[:,0],shape=(N,1))\n",
    "t1 = tf.reshape(Theta[:,1],shape=(N,1))\n",
    "t2 = tf.reshape(Theta[:,2],shape=(N,1))\n",
    "\n",
    "\n",
    "# Model output\n",
    "x20 = tf.reduce_mean(x0*x0)\n",
    "x10 = tf.reduce_mean(x0)\n",
    "x00 = 1.\n",
    "Z0 = x20*t2 + x10*t1 + x00*t0    \n",
    "Y = Z0 + 0.001*tf.random.normal((N,1)) #target vector/ Data Outputs with some noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Netwon's type methods\n",
    "\n",
    "class newton_opt:\n",
    "    \"\"\"\n",
    "    x0: intial param\n",
    "    fcn: returns error vector (fitting error )\n",
    "    alpha: learning rate or regularizer\n",
    "    method: GD(Gradient Descent),\n",
    "            GN(Gauss Newton),\n",
    "            LM(Levenberg-Marquardt),\n",
    "            EN(Exact NEwton),\n",
    "            BFGS  \n",
    "    global_: if True, uses line-search based on Armijo-Backtracking algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x0, fcn, method=\"GD\", global_=False, alpha=0.0001, eps=1E-3):\n",
    "        self.eps = eps\n",
    "        self.x = tf.Variable(tf.reshape(x0,(-1,1)))\n",
    "        self.m = tf.shape(x0).numpy()[0]\n",
    "        self.f = fcn\n",
    "        self.alpha = alpha\n",
    "        self.method = method # Optimization Method\n",
    "        self.global_ = global_\n",
    "        if self.method in [\"GN\"]:\n",
    "            self.alpha = 0. # GN is a special case of LM with alpha=0\n",
    "        if self.method in [\"GD\",\"EN\",\"BFGS\"]:\n",
    "            self.f = self.f_sq_err(fcn) #Use the least square cost directly\n",
    "        \n",
    "        self.fs = self.f_sq_err(fcn)\n",
    "            \n",
    "       \n",
    "    def jacob(self, x):\n",
    "        \"\"\"Returns the Jacobian of the objective function\"\"\"\n",
    "        with tf.GradientTape() as t:\n",
    "            y = self.f(x)\n",
    "        dy_dx = t.jacobian(y,x)\n",
    "        return tf.reshape(dy_dx,(-1,self.m))\n",
    " \n",
    "    def f_sq_err(self,fcn):\n",
    "        \"\"\"Return a function which gives norm of error vector\"\"\"\n",
    "        def f_sq(x=self.x):\n",
    "            err_ = fcn(x)\n",
    "            output = tf.reduce_sum(err_*err_)\n",
    "            return output\n",
    "        return f_sq\n",
    "\n",
    "    def hess(self, x):\n",
    "        \"\"\"Returns the Hessian of scalar function \"\"\"\n",
    "        with tf.GradientTape() as t:\n",
    "            with tf.GradientTape() as t2:\n",
    "                y = self.f(x)\n",
    "                assert not (tf.size(y)>1), \"The obj fcn has to be scalar valued!!\"\n",
    "            dy_dx = t2.jacobian(y, x)\n",
    "        d2y_dx2 = t.jacobian(dy_dx, x)\n",
    "        return tf.reshape(d2y_dx2,(self.m,self.m))\n",
    "    \n",
    "    \n",
    "    def optimize(self, max_it=100):\n",
    "        i = 0\n",
    "        B = 1*tf.eye(self.m) #intialization for BFGS\n",
    "        while True: \n",
    "            if self.method in [\"GD\",\"EN\",\"BFGS\"]:\n",
    "                gradf = tf.transpose(self.jacob(self.x))\n",
    "            elif self.method in [\"GN\",\"LM\"]:\n",
    "                F = self.f(self.x)\n",
    "                J = self.jacob(self.x) # shape=(N,m)\n",
    "                Jt = tf.transpose(J)\n",
    "                gradf = tf.matmul(Jt,F)\n",
    "\n",
    "            \n",
    "            if self.method ==\"GD\":\n",
    "                B = (1/self.alpha)*tf.eye(self.m)\n",
    "\n",
    "            elif self.method in [\"GN\", \"LM\"]:\n",
    "                B = tf.matmul(Jt,J)+self.alpha*tf.eye(self.m)\n",
    "            \n",
    "            elif self.method==\"EN\":\n",
    "                B = self.hess(self.x)    \n",
    "\n",
    "            invB = tf.linalg.pinv(B)\n",
    "            \n",
    "            # Step size \n",
    "            dx = -tf.matmul(invB,gradf)\n",
    "            \n",
    "            # Line search based on Armijo condition and backtracking\n",
    "            if self.global_ == True:\n",
    "                gamma_ = 0.1 # choose in (0,1)\n",
    "                beta_ = 0.8 # choose in (0,1)\n",
    "                t = 1. #\n",
    "                while self.fs(self.x+t*dx) >= (self.fs(self.x)+gamma_*t*tf.reduce_sum(tf.multiply(gradf, dx))) or t < 0.1:\n",
    "                    t=beta_*t\n",
    "                dx = t*dx\n",
    "                \n",
    "                \n",
    "            \n",
    "\n",
    "            # Update\n",
    "            xold = tf.Variable(self.x)\n",
    "            self.x = tf.Variable(self.x+dx)\n",
    "            \n",
    "            if self.method == \"BFGS\":\n",
    "                s = dx\n",
    "                st = tf.transpose(s)\n",
    "                yt = self.jacob(self.x)-self.jacob(xold)\n",
    "                y = tf.transpose(yt)\n",
    "                c1 = tf.matmul(tf.matmul(st,B),s)\n",
    "                c2 = tf.matmul(st,y)\n",
    "                Bs = tf.matmul(B,s)\n",
    "                B = B - (1/c1)*tf.matmul(Bs,tf.transpose(Bs)) + (1/c2)*tf.matmul(y,yt)\n",
    "             \n",
    "            # Exit condition\n",
    "            i = i+1\n",
    "\n",
    "            if np.linalg.norm(dx.numpy())<self.eps or i>max_it :\n",
    "                err_ = self.f(self.x).numpy().reshape(-1,)\n",
    "                print(\"Converges in iteraton: \",i)\n",
    "                print(\"Fitting Error: \", np.dot(err_,err_)**(0.5))\n",
    "                if i>max_it :\n",
    "                    print(\"Warning: Maximum iteration crossed. \", \n",
    "                          \"Algorithm may not have converged. Increase max_it\")\n",
    "                break\n",
    "\n",
    "        return self.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_vec_err(x):\n",
    "    \"\"\"\n",
    "    Returns the error vector from the forward map/model.\n",
    "    Can use any nonlinear function.\n",
    "    \n",
    "    params:\n",
    "        x: the optimization parameter \n",
    "        return:  error vector=(Y-model(x))\n",
    "        \n",
    "    \"\"\"\n",
    "    x2_ = tf.reduce_mean(x*x)\n",
    "    x1_ = tf.reduce_mean(x)\n",
    "    x0_ =1\n",
    "    Z = x2_*t2+x1_*t1+x0_*t0  \n",
    "    err_ = Y-Z\n",
    "    output =  tf.reshape(err_,(N,1))\n",
    "    \n",
    "    return output\n",
    " \n",
    "# def f_sq_err(x, fcn=f_vec_err):\n",
    "#     \"\"\"\n",
    "#     Define the forward map of the objective function.\n",
    "#     Can use any nonlinear function\n",
    "    \n",
    "#     params:\n",
    "#         fcn: Returns error vector for the current estimate\n",
    "#         x: the optimization parameter\n",
    "#     \"\"\"\n",
    "#     err_ = fcn(x)\n",
    "\n",
    "#     output = tf.reduce_sum(err_*err_)\n",
    "        \n",
    "#     return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the parameter x \n",
    "x0_ = tf.Variable(tf.random.uniform((m,1),minval=-1, maxval=1)) \n",
    "eps_ = 1E-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converges in iteraton:  6\n",
      "Fitting Error:  0.013295533978667616\n"
     ]
    }
   ],
   "source": [
    "opt = newton_opt(x0=x0_, fcn=f_vec_err, method=\"GN\", eps=eps_)\n",
    "x = opt.optimize(max_it=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converges in iteraton:  16\n",
      "Fitting Error:  0.013295434926366598\n"
     ]
    }
   ],
   "source": [
    "opt = newton_opt(x0=x0_, fcn=f_vec_err, method=\"LM\", alpha= 0.1, eps=eps_)\n",
    "x = opt.optimize(max_it=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converges in iteraton:  43\n",
      "Fitting Error:  0.00017677122855504532\n"
     ]
    }
   ],
   "source": [
    "opt = newton_opt(x0=x0_, fcn=f_vec_err, method=\"BFGS\", eps=eps_)\n",
    "x = opt.optimize(max_it=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converges in iteraton:  11\n",
      "Fitting Error:  0.00017677122855504532\n"
     ]
    }
   ],
   "source": [
    "opt = newton_opt(x0=x0_, fcn=f_vec_err, method=\"EN\", eps=eps_)\n",
    "x = opt.optimize(max_it=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converges in iteraton:  101\n",
      "Fitting Error:  0.307903652927043\n",
      "Warning: Maximum iteration crossed.  Algorithm may not have converged. Increase max_it\n"
     ]
    }
   ],
   "source": [
    "opt = newton_opt(x0=x0_, fcn=f_vec_err, method=\"GD\", alpha=0.01, eps=eps_)\n",
    "x = opt.optimize(max_it=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
